{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dane-meister/Machine-Learning-Algos/blob/main/reinforce_%26_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_v04AsuPAA"
      },
      "source": [
        "# REINFORCE and Actor-Critic\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The goal is to implement REINFORCE and Actor-Critic\n",
        "\n",
        "We will be using 4 Gymnasium environments (InvertedPendulum-v4, Hopper-v4, HalfCheetah-v4, and image-based InvertedPendulum-v4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAKjHf2vs1tP"
      },
      "outputs": [],
      "source": [
        "!apt install xvfb -y\n",
        "!pip install gymnasium[classic-control,atari,accept-rom-license,mujoco]\n",
        "!pip install opencv-python\n",
        "%env MUJOCO_GL=egl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRI7aIiMywhA"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from io import BytesIO\n",
        "\n",
        "# Code for visualizing the episode\n",
        "\n",
        "class GIFMaker:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.images = []\n",
        "        self.buffer = BytesIO()\n",
        "\n",
        "    def append(self, img):\n",
        "        self.images.append(img)\n",
        "\n",
        "    def display(self):\n",
        "        imageio.mimsave(self.buffer, self.images, format='gif')\n",
        "        gif = Image(data=self.buffer.getvalue())\n",
        "        display(gif)\n",
        "        return gif\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import packages we need"
      ],
      "metadata": {
        "id": "iTs3XitgmSyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "import cv2"
      ],
      "metadata": {
        "id": "S-WBeRDkmZzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWjhkm1uV7N"
      },
      "source": [
        "## Env1 InvertedPendulum\n",
        "Detailed information of this environment:\n",
        "https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L5xY5Rltb2o"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
        "g = GIFMaker() # visualization\n",
        "observation, info = env.reset(seed=42)\n",
        "for i in range(500):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # print(i, action, observation, reward)\n",
        "    g.append(env.render()) # save one frame\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "g.display() # show GIF animation\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brcs8ybULznD"
      },
      "source": [
        "## Env2 Hopper-v4\n",
        "Detailed information of this environment:\n",
        "https://gymnasium.farama.org/environments/mujoco/hopper/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WioKFCKXLtwR"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Hopper-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "observation, info = env.reset(seed=42)\n",
        "g = GIFMaker()\n",
        "for i in range(200):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # print(i, action, observation, reward)\n",
        "    g.append(env.render())\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "g.display()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zG0W9NjWGu"
      },
      "source": [
        "## Env3 HalfCheetah-v4\n",
        "Detailed information of this environment: https://gymnasium.farama.org/environments/mujoco/half_cheetah/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLJpOasGjW2V"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"HalfCheetah-v4\", render_mode=\"rgb_array\")\n",
        "\n",
        "observation, info = env.reset(seed=42)\n",
        "g = GIFMaker()\n",
        "for i in range(200):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # print(i, action, observation, reward)\n",
        "    g.append(env.render())\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "g.display()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3AT79O_uxyT"
      },
      "source": [
        "## Env4 *Image-based* InvertedPendulum\n",
        "Detailed information of this environment: https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXsQceAQu-KA"
      },
      "outputs": [],
      "source": [
        "from gymnasium import ObservationWrapper\n",
        "from gymnasium.wrappers import (\n",
        "    PixelObservationWrapper,\n",
        "    GrayScaleObservation,\n",
        "    ResizeObservation\n",
        ")\n",
        "from gymnasium.spaces import Box\n",
        "\n",
        "class KeyObservationWrapper(ObservationWrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.observation_space = self.observation_space['pixels']\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return observation['pixels']\n",
        "\n",
        "class MakeChannelObservationWrapper(ObservationWrapper):\n",
        "    def __init__(self, env: gym.Env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.observation_space = Box(\n",
        "            low = self.observation_space.low[np.newaxis, :],\n",
        "            high = self.observation_space.high[np.newaxis, :],\n",
        "            shape = (1,) + self.observation_space.shape,\n",
        "            dtype = self.observation_space.dtype\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return observation[np.newaxis, :]\n",
        "\n",
        "\n",
        "def make_image_env(env_name):\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    env = PixelObservationWrapper(env)\n",
        "    env = KeyObservationWrapper(env)\n",
        "    env = GrayScaleObservation(env)\n",
        "    env = ResizeObservation(env, (84, 84))\n",
        "    env = MakeChannelObservationWrapper(env)\n",
        "    return env\n",
        "\n",
        "env = make_image_env(\"InvertedPendulum-v4\")\n",
        "observation, info = env.reset(seed=42)\n",
        "print (observation.shape)\n",
        "g = GIFMaker()\n",
        "for i in range(200):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # this observation gives you (1, 84, 84), 1 means one gray channel\n",
        "    # print(i, action, observation, reward)\n",
        "    # g.append(env.render()) # original rgb frames\n",
        "    g.append(observation.squeeze(0))    # see the image observations\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "g.display()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5fPlLjSW7y"
      },
      "source": [
        "##REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egLnG4J4SWDd"
      },
      "outputs": [],
      "source": [
        "# The policy network for numerical states using MLP\n",
        "class Policy(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim\n",
        "    ):\n",
        "        super(Policy, self).__init__()\n",
        "        # print(state_dim, action_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        x = F.tanh(x) # scale the final output to [-1, 1]; And we treat it as the mean value of a (multi-variable) normal distribution\n",
        "        return x\n",
        "\n",
        "# The policy network for **image observations** using CNN\n",
        "class Policy_Image(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_channels,\n",
        "        action_dim\n",
        "    ):\n",
        "        super(Policy_Image, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(image_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.fc2 = nn.Linear(512, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        # Flatten the output for the linear layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = F.tanh(x) # scale the final output to [-1, 1]; And we treat it as the mean value of a (multi-variable) normal distribution\n",
        "        return x\n",
        "\n",
        "\n",
        "# REINFORCE\n",
        "class REINFORCE():\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        env_name,\n",
        "        policy_network,\n",
        "        var=0.1,\n",
        "        var_decay_steps=20000,\n",
        "        gamma=0.95,\n",
        "        lr=0.01\n",
        "    ):\n",
        "        # initlize env and env name\n",
        "        self.env = env\n",
        "        self.env_name = env_name\n",
        "\n",
        "        # set state dimension and number of actions\n",
        "        self.state_dim = env.observation_space.shape\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "\n",
        "        # create policy network:\n",
        "        if len(self.state_dim) > 1:\n",
        "            self.policy_network = Policy_Image(image_channels=self.state_dim[0], action_dim=self.action_dim)\n",
        "        else:\n",
        "            # for numerical states\n",
        "            self.state_dim = self.state_dim[0]\n",
        "            self.policy_network = Policy(state_dim=self.state_dim, action_dim=self.action_dim)\n",
        "\n",
        "        # assign gpu if avliable\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # move the models to gpu\n",
        "        self.policy_network = self.policy_network.to(self.device)\n",
        "\n",
        "        # (end) variance value for normal distribution\n",
        "        self.var = var\n",
        "        # how many steps epsilon decays from 1 to eps\n",
        "        self.var_decay_steps = var_decay_steps\n",
        "\n",
        "        # set optimizer and make it use \"lr\" (the learning rate)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
        "\n",
        "        # mode is used to switch between optimization and Evaluation\n",
        "        self.mode = \"eval\"\n",
        "        self.total_transitions = 0\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.g = None\n",
        "\n",
        "\n",
        "    # function to choose action given a state\n",
        "    # return the log probablity of the action as well\n",
        "    # return values are all tensors (on gpu if using gpu)\n",
        "    def act(self, state):\n",
        "        if self.mode == \"eval\":\n",
        "            with torch.no_grad():\n",
        "                # For evaluation mode\n",
        "                action = self.policy_network(state)#.squeeze()\n",
        "                log_prob = torch.zeros_like(action) #all zeros #torch.tensor([0.0])\n",
        "\n",
        "                return action, log_prob\n",
        "\n",
        "        elif self.mode == \"train\":\n",
        "            # variance of the policy\n",
        "            # which decays from the initial high value (1) to final value (self.var)\n",
        "            # this is similar to epsilon decay in DQN\n",
        "            if self.total_transitions < self.var_decay_steps:\n",
        "                var = (self.var_decay_steps-self.total_transitions) / self.var_decay_steps * (1 - self.var) + self.var\n",
        "            else:\n",
        "                var = self.var\n",
        "\n",
        "            # we create a normal distribution for action\n",
        "            # where the mean is the output from the policy network\n",
        "            # the variance is a constant value\n",
        "            mean = self.policy_network(state) # mean\n",
        "            var = torch.full((self.action_dim,), var).to(self.device) # variance\n",
        "            normal_dist = Normal(mean, var.sqrt())\n",
        "\n",
        "            # sample from the normal distribution as the action\n",
        "            action = normal_dist.sample()\n",
        "\n",
        "            # let us clip the action to range [-1, 1] in case environment does not accept that\n",
        "            action = torch.clip(action, -1, 1)\n",
        "\n",
        "            # get the log probablity of that action according to the normal distribution\n",
        "            log_prob = normal_dist.log_prob(action)\n",
        "\n",
        "        return action, log_prob\n",
        "\n",
        "    # REINFORCE update\n",
        "    def optimization(self, log_probs, returns):\n",
        "        # make the model in train mode\n",
        "        self.policy_network.train()\n",
        "\n",
        "        log_probs = log_probs.mean(dim=1).unsqueeze(1)\n",
        "        loss = -(log_probs * returns).sum()\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    # run for one episode\n",
        "    def run_one_episode(self, visualize=False):\n",
        "        self.g = GIFMaker()\n",
        "\n",
        "        terminated = False\n",
        "        r = 0\n",
        "\n",
        "        # initialize lists for holding rewards and log_probs we experienced\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "\n",
        "        # reset the environment\n",
        "        state, info = self.env.reset()\n",
        "\n",
        "        if visualize:\n",
        "            self.g.append(env.render())\n",
        "\n",
        "        while not terminated:\n",
        "            # convert state to tensor, make a batch dimension and move it to gpu\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # choose (sample) an action using self.act()\n",
        "            # also get its log probability\n",
        "            action_tensor, log_prob_tensor = self.act(state_tensor)\n",
        "\n",
        "            # convert the action tensor to a number for taking a step in environment\n",
        "            action = action_tensor.cpu().numpy()[0]\n",
        "\n",
        "            # Go to next state\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "            r += reward # note: r is cumulative reward and reward is this step's reward\n",
        "\n",
        "\n",
        "            # append the log probability of action to the list for later optimization\n",
        "            # append this step's reward to rewards list, which will be used to compute future rewards for optimization\n",
        "            log_probs.append(log_prob_tensor)\n",
        "            rewards.append(torch.tensor([reward], device=self.device))\n",
        "\n",
        "            # for rendering\n",
        "            if visualize:\n",
        "                self.g.append(env.render())\n",
        "\n",
        "            # inc total_transitions\n",
        "            if self.mode == \"train\":\n",
        "                self.total_transitions += 1\n",
        "\n",
        "            # Terminated then return\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # optimize your policy after this episode ends\n",
        "        if self.mode == \"train\":\n",
        "            future_rewards = []\n",
        "            for r in reversed(rewards):\n",
        "                if future_rewards:\n",
        "                    future_rewards.insert(0, r + self.gamma * future_rewards[0])\n",
        "                else:\n",
        "                    future_rewards.insert(0, r)\n",
        "            future_rewards = torch.cat(future_rewards).unsqueeze(1)\n",
        "\n",
        "            log_probs = torch.cat(log_probs)\n",
        "\n",
        "            # optimize your policy by this trajectory\n",
        "            self.optimization(log_probs, future_rewards)\n",
        "\n",
        "        return r\n",
        "\n",
        "    def train(self):\n",
        "        self.mode = \"train\"\n",
        "        self.run_one_episode()\n",
        "\n",
        "    # Evaluation of policy\n",
        "    def eval(self, n, visualize=False):\n",
        "        self.mode = \"eval\" # put in eval mode\n",
        "        returns = []\n",
        "        for i in range(n):                          # run evaluation for n episode\n",
        "            returns.append(self.run_one_episode(visualize))\n",
        "        return np.mean(returns)                    # return average returns over niter episodes\n",
        "\n",
        "\n",
        "    # the function called to perform optimization and evaluation\n",
        "    def execute(self, total_ep=5000, eval_freq=100, eval_ep=100):\n",
        "        rewards = []   #used to track polciy evaluation across runs\n",
        "        episodes = []  #number of episodes used to update policy\n",
        "\n",
        "        prog_bar = tqdm(range(0, total_ep))\n",
        "        for i in prog_bar:\n",
        "            self.train()                        # train\n",
        "            if (i+1) % eval_freq == 0:          # evaluate using eval_ep episodes every eval_freq policy updates\n",
        "                reward = self.eval(eval_ep)\n",
        "                rewards.append(reward)\n",
        "                episodes.append(i)\n",
        "                print (f\"Eval Reward: {reward:.2f} at ep {i+1}\")\n",
        "\n",
        "        plt.plot(episodes, rewards)   #plot evaluation reward vs episodes\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('REINFORCE on '+self.env_name)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize(self):\n",
        "        self.g.display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTPuXmz8nYal"
      },
      "outputs": [],
      "source": [
        "# InvertedPendulum-v4 REINFORCE\n",
        "gc.collect() # free some unused RAM, don't worry about this\n",
        "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
        "reinforce = REINFORCE(env, \"InvertedPendulum\", Policy, var=0.01, lr=1e-3, gamma=0.95) # you can explore different gamma, var (variance of your policy) and learning rate\n",
        "reinforce.execute(total_ep=12000, eval_freq=400, eval_ep=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the final policy (could be slow because it renders images. Be patient)\n",
        "print (f\"Final Eval: {reinforce.eval(20, visualize=True):.2f}\")\n",
        "reinforce.visualize()"
      ],
      "metadata": {
        "id": "qV4ej0QRio5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHvZ7aFE5yhI"
      },
      "outputs": [],
      "source": [
        "# Hopper-v4 REINFORCE\n",
        "gc.collect() # free some unused RAM, don't worry about this\n",
        "env = gym.make(\"Hopper-v4\", render_mode=\"rgb_array\", max_episode_steps=500)\n",
        "reinforce = REINFORCE(env, \"Hopper\", Policy, var=0.05, lr=1e-4, gamma=0.99) # you can explore different gamma, var (variance of your policy) and learning rate\n",
        "reinforce.execute(total_ep=3000, eval_freq=50, eval_ep=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCf_1f2gw8-B"
      },
      "outputs": [],
      "source": [
        "# evaluate the final policy (could be slow because it renders images. Be patient)\n",
        "print (f\"Final Eval: {reinforce.eval(10, visualize=True):.2f}\")\n",
        "reinforce.visualize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFP6NN5yLDgF"
      },
      "outputs": [],
      "source": [
        "# HalfCheetah-v4 REINFORCE\n",
        "gc.collect() # free some unused RAM\n",
        "env = gym.make(\"HalfCheetah-v4\", render_mode=\"rgb_array\", max_episode_steps=500)\n",
        "reinforce = REINFORCE(env, \"HalfCheetah\", Policy, var=0.05, lr=1e-3, gamma=0.95) # you can explore different gamma, var (variance of your policy) and learning rate\n",
        "reinforce.execute(total_ep=2000, eval_freq=50, eval_ep=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2VagKOnF9FV"
      },
      "outputs": [],
      "source": [
        "# evaluate the final policy (could be slow because it renders images. Be patient)\n",
        "print (f\"Final Eval: {reinforce.eval(10, visualize=True):.2f}\")\n",
        "reinforce.visualize()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image based InvertedPendulum-v4\n",
        "env = make_image_env(\"InvertedPendulum-v4\")\n",
        "gc.collect()\n",
        "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
        "reinforce = REINFORCE(env, \"InvertedPendulum\", Policy_Image, var=0.03, lr=1e-4, gamma=0.95) # you can explore different gamma, var (variance of your policy) and learning rate\n",
        "reinforce.execute(total_ep=5000, eval_freq=200, eval_ep=10)"
      ],
      "metadata": {
        "id": "avMSy0EtlFjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1oT97j8jKCp"
      },
      "source": [
        "#Actor Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeUzjPXJjJAq"
      },
      "outputs": [],
      "source": [
        "# The actor network for numerical states using MLP\n",
        "class Actor(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim\n",
        "    ):\n",
        "        super(Actor, self).__init__()\n",
        "        # print(state_dim, action_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        x = F.tanh(x) # scale the output to [-1, 1]; And we treat it as the mean value of a (multi-variable) normal distribution\n",
        "        return x\n",
        "\n",
        "# The actor network for **image states** using CNN\n",
        "class Actor_Image(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_channels,\n",
        "        action_dim\n",
        "    ):\n",
        "        super(Actor_Image, self).__init__()\n",
        "        # print(state_dim, action_dim)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(image_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.fc2 = nn.Linear(512, action_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = F.tanh(x) # scale the output to [-1, 1]; And we treat it as the mean value of a (multi-variable) normal distribution\n",
        "        return x\n",
        "\n",
        "\n",
        "# The critic network for numerical states using MLP\n",
        "class Critic(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim\n",
        "    ):\n",
        "        super(Critic, self).__init__()\n",
        "        # print(state_dim, action_dim)\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)  # Outputs a single value (state value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No activation here, as we want a raw score\n",
        "        return x\n",
        "\n",
        "# The critic network for **image states** using CNN\n",
        "class Critic_Image(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_channels\n",
        "    ):\n",
        "        super(Critic_Image, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(image_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 512)  # Adjust based on the output of conv layers\n",
        "        self.fc2 = nn.Linear(512, 1)  # Outputs a single value\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # No activation here\n",
        "        return x\n",
        "\n",
        "\n",
        "# ActorCritic\n",
        "class ActorCritic():\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        env_name,\n",
        "        actor,\n",
        "        critic,\n",
        "        var=0.1,\n",
        "        var_decay_steps=20000,\n",
        "        train_freq=4,\n",
        "        target_network_update_freq=200,\n",
        "        gamma=0.95,\n",
        "        lr=1e-3\n",
        "    ):\n",
        "        # initlize env and env name\n",
        "        self.env = env\n",
        "        self.env_name = env_name\n",
        "\n",
        "        # set state dimension and number of actions\n",
        "        self.state_dim = env.observation_space.shape\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "\n",
        "        # create actor and critic networks:\n",
        "        if len(self.state_dim) > 1:\n",
        "            self.actor = Actor_Image(image_channels=self.state_dim[0], action_dim=self.action_dim)\n",
        "            self.critic = Critic_Image(image_channels=self.state_dim[0])\n",
        "            self.critic_target = Critic_Image(image_channels=self.state_dim[0])\n",
        "        else:\n",
        "            # for numerical states\n",
        "            self.state_dim = self.state_dim[0]\n",
        "            self.actor = Actor(state_dim=self.state_dim, action_dim=self.action_dim)\n",
        "            self.critic = Critic(state_dim=self.state_dim)\n",
        "            self.critic_target = Critic(state_dim=self.state_dim)\n",
        "\n",
        "        # initialize the target critic network using the weights from critic network\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        # assign gpu if avliable\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # move the models to gpu\n",
        "        self.actor = self.actor.to(self.device)\n",
        "        self.critic = self.critic.to(self.device)\n",
        "        self.critic_target = self.critic_target.to(self.device)\n",
        "\n",
        "        # (end) variance value for normal distribution\n",
        "        self.var = var\n",
        "        # how many steps epsilon decays from 1 to eps\n",
        "        self.var_decay_steps = var_decay_steps\n",
        "\n",
        "        # set optimizers, one each network\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
        "\n",
        "        # mode is used to switch between optimization of DQN and Evaluation\n",
        "        self.mode = \"eval\"\n",
        "        self.total_transitions = 0\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.g = None\n",
        "        self.train_freq = train_freq\n",
        "        self.target_network_update_freq = target_network_update_freq\n",
        "\n",
        "\n",
        "    # function to choose action given a state\n",
        "    # returns the log probablity of the action as well\n",
        "    # returns values are all tensors (on gpu if using gpu)\n",
        "    def act(self, state):\n",
        "        if self.mode == \"eval\":\n",
        "            with torch.no_grad():\n",
        "                mean = self.actor(state)\n",
        "                action = torch.tanh(mean)\n",
        "                log_prob = torch.zeros_like(action)\n",
        "\n",
        "                return action, log_prob\n",
        "\n",
        "        elif self.mode == \"train\":\n",
        "            # variance of the policy\n",
        "            # which decays from the initial high value (1) to final value (self.var)\n",
        "            # this is similar to epsilon decay in DQN\n",
        "            if self.total_transitions < self.var_decay_steps:\n",
        "                var = (self.var_decay_steps-self.total_transitions) / self.var_decay_steps * (1 - self.var) + self.var\n",
        "            else:\n",
        "                var = self.var\n",
        "\n",
        "            # we create a normal distribution for action\n",
        "            # where the mean is the output from the policy network\n",
        "            # the variance is a constant value\n",
        "            mean = self.actor(state) # mean\n",
        "            var = torch.full((self.action_dim,), self.var).to(self.device)  # variance\n",
        "            normal_dist = Normal(mean, var.sqrt())\n",
        "\n",
        "            # sample from the normal distribution as the action\n",
        "            action = normal_dist.sample()\n",
        "\n",
        "            # let us clip the action to range [-1, 1] in case environment does not accept that\n",
        "            action = torch.clip(action, -1, 1)\n",
        "\n",
        "            # get the log probablity of that action according to the normal distribution\n",
        "            log_prob = normal_dist.log_prob(action)\n",
        "\n",
        "        return action, log_prob\n",
        "\n",
        "    # optimization\n",
        "    def optimization(self, log_probs, states, next_states, rewards, not_dones):\n",
        "        # put the models into training mode\n",
        "        self.actor.train()\n",
        "        self.critic.train()\n",
        "\n",
        "        log_probs = torch.cat(log_probs, dim=0)\n",
        "        states = torch.cat(states)\n",
        "        next_states = torch.cat(next_states)\n",
        "        rewards = torch.cat(rewards)\n",
        "        not_dones = torch.cat(not_dones)\n",
        "\n",
        "        # compute value v(s) and v(s') using critic and advantages\n",
        "        values = self.critic(states)\n",
        "        with torch.no_grad():\n",
        "          next_values = self.critic_target(next_states)\n",
        "\n",
        "        # compute actor loss\n",
        "        # Taking the mean across the action dimension (dim=1)\n",
        "        log_probs_mean = log_probs.mean(dim=1)\n",
        "        advantages = rewards + self.gamma * next_values * not_dones - values\n",
        "        # Detaching advantages to prevent gradient flow into critic\n",
        "        advantages_detached = advantages.detach()\n",
        "        actor_loss = -(log_probs_mean * advantages_detached).mean()\n",
        "\n",
        "        # optimize the actor using actor loss\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # compute critic loss\n",
        "        target_values = rewards + self.gamma * next_values * not_dones\n",
        "        critic_loss = F.mse_loss(values, target_values)\n",
        "\n",
        "        # optimize the critic network (not the target critic!)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # update the target critic network based on critic network paramters every self.target_network_update_freq steps\n",
        "        if self.total_transitions % self.target_network_update_freq == 0:\n",
        "          self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        # ======================================\n",
        "\n",
        "\n",
        "    # run for one episode\n",
        "    def run_one_episode(self, visualize=False):\n",
        "        self.g = GIFMaker()\n",
        "\n",
        "        terminated = False\n",
        "        r = 0\n",
        "\n",
        "        # make empty lists to hold state, next state, stepwise reward, log probabilities, and not_done\n",
        "        states = []\n",
        "        next_states = []\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        not_dones = []\n",
        "\n",
        "\n",
        "        # reset\n",
        "        state, info = self.env.reset()\n",
        "\n",
        "        if visualize:\n",
        "            self.g.append(env.render())\n",
        "\n",
        "        while not terminated:\n",
        "            # make state to tensor, add a batch dimension, and move it to gpu\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # select (sample) an action and get its log probability\n",
        "            action_tensor, log_prob_tensor = self.act(state_tensor)\n",
        "\n",
        "\n",
        "            # convert action to numpy array for using it in environment\n",
        "            action = action_tensor.cpu().numpy()[0]\n",
        "\n",
        "            # go to next state using that action\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "            r += reward\n",
        "\n",
        "            # for rendering\n",
        "            if visualize:\n",
        "                self.g.append(env.render())\n",
        "\n",
        "            # create tensors for next_state, reward, and not_done\n",
        "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "            reward_tensor = torch.tensor([reward], dtype=torch.float32).to(self.device)\n",
        "            not_done_tensor = torch.tensor([1.0 - float(terminated)], device=self.device)\n",
        "\n",
        "            if self.mode == \"train\":\n",
        "                self.total_transitions += 1\n",
        "\n",
        "                # if training, append the transition (log probabilities, states, next_states, rewards and not dones)\n",
        "                # to the lists holding them\n",
        "                states.append(state_tensor)\n",
        "                next_states.append(next_state_tensor)\n",
        "                rewards.append(reward_tensor)\n",
        "                log_probs.append(log_prob_tensor)\n",
        "                not_dones.append(not_done_tensor)\n",
        "\n",
        "                # every time we need to train or the trajectory ends\n",
        "                if (self.total_transitions % self.train_freq) or terminated or truncated:\n",
        "\n",
        "                    # call optimization\n",
        "                    self.optimization(log_probs, states, next_states, rewards, not_dones)\n",
        "\n",
        "                    # reset the lists for holding transitions back to empty lists\n",
        "                    states, next_states, rewards, log_probs, not_dones = [], [], [], [], []\n",
        "\n",
        "            # terminated then return\n",
        "            if terminated or truncated:\n",
        "                return r\n",
        "\n",
        "            # don't forget to assign next_state to state for the next loop\n",
        "            state = next_state\n",
        "\n",
        "    def train(self):\n",
        "        self.mode = \"train\"\n",
        "        self.run_one_episode()\n",
        "\n",
        "    # Evaluation of policy\n",
        "    def eval(self, n, visualize=False):\n",
        "        self.mode = \"eval\" # put in eval mode\n",
        "        returns = []\n",
        "        for i in range(n):                          # run evaluation for n episode\n",
        "            returns.append(self.run_one_episode(visualize))\n",
        "        return np.mean(returns)                    # return average returns over niter episodes\n",
        "\n",
        "\n",
        "    # the function called to perform optimization and evaluation\n",
        "    def execute(self, total_ep=5000, eval_freq=100, eval_ep=100):\n",
        "        rewards = []   #used to track polciy evaluation across runs\n",
        "        episodes = []  #number of episodes used to update policy\n",
        "\n",
        "        prog_bar = tqdm(range(0, total_ep))\n",
        "        for i in prog_bar:\n",
        "            self.train()                        # train\n",
        "            if (i+1) % eval_freq == 0:          # evaluate using eval_ep episodes every eval_freq policy updates\n",
        "                reward = self.eval(eval_ep)\n",
        "                rewards.append(reward)\n",
        "                episodes.append(i)\n",
        "                print (f\"Eval Reward: {reward:.2f} at ep {i+1}\")\n",
        "\n",
        "        plt.plot(episodes, rewards)   #plot evaluation reward vs episodes\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('Actor Critic on '+self.env_name)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize(self):\n",
        "        self.g.display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBxSu2jGLD08"
      },
      "outputs": [],
      "source": [
        "# InvertedPendulum-v4 Actor-Critic\n",
        "gc.collect() # free some unused RAM, don't worry about this\n",
        "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
        "ac = ActorCritic(env, \"InvertedPendulum\", Actor, Critic, var=0.06, lr=1e-3, train_freq=64) # you can explore different gamma, var (variance of your policy) and learning rate\n",
        "ac.execute(total_ep=5000, eval_freq=200, eval_ep=20) # run at least 5000 episodes # roughly around 25mins if total_ep=5000, eval_freq=200, eval_ep=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BFztns507ij"
      },
      "outputs": [],
      "source": [
        "# evaluate the final policy (could be slow because it renders images. Be patient)\n",
        "print (f\"Final Eval: {ac.eval(20, visualize=True):.2f}\")\n",
        "ac.visualize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXR-Fm7DLRlC"
      },
      "outputs": [],
      "source": [
        "# Hopper-v4 Actor-Critic\n",
        "gc.collect() # free some unused RAM, don't worry about this\n",
        "env = gym.make(\"Hopper-v4\", render_mode=\"rgb_array\")\n",
        "ac = ActorCritic(env, \"Hopper\", Actor, Critic, var=0.04, lr=1e-4, train_freq=32) # you can explore different gamma, var (variance of your policy) and learning rate\n",
        "ac.execute(total_ep=5000, eval_freq=200, eval_ep=10) # run at least 5000 episodes # roughly around 20mins if total_ep=5000, eval_freq=200, eval_ep=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSYztyJ80961"
      },
      "outputs": [],
      "source": [
        "# evaluate the final policy (could be slow because it renders images. Be patient)\n",
        "print (f\"Final Eval: {ac.eval(10, visualize=True):.2f}\")\n",
        "ac.visualize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHJu4DCv6JTW"
      },
      "outputs": [],
      "source": [
        "# HalfCheetah-v4 Actor-Critic\n",
        "gc.collect() # free some unused RAM, don't worry about this\n",
        "env = gym.make(\"HalfCheetah-v4\", render_mode=\"rgb_array\")\n",
        "ac = ActorCritic(env, \"HalfCheetah\", Actor, Critic, var=0.08, lr=1e-3, train_freq=32, gamma=0.99) # you can explore different gamma, var (variance of your policy) and learning rate\n",
        "ac.execute(total_ep=850, eval_freq=50, eval_ep=10) # run at least 600 episodes # roughly around 50mins if total_ep=600, eval_freq=50, eval_ep=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnnWbK_56MiD"
      },
      "outputs": [],
      "source": [
        "# evaluate the final policy (could be slow because it renders images. Be patient)\n",
        "print (f\"Final Eval: {ac.eval(10, visualize=True):.2f}\")\n",
        "ac.visualize()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image based InvertedPendulum-v4 Actor-Critic\n",
        "env = make_image_env(\"InvertedPendulum-v4\")\n",
        "gc.collect() # free some unused RAM, don't worry about this\n",
        "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"rgb_array\")\n",
        "ac = ActorCritic(env, \"InvertedPendulum\", Actor_Image, Critic_Image, var=0.02, lr=1e-3, train_freq=32, gamma=0.92) # you can explore different gamma, var (variance of your policy) and learning rate\n",
        "ac.execute(total_ep=5000, eval_freq=200, eval_ep=10) # please train for at least 5000 episodes"
      ],
      "metadata": {
        "id": "pfUFnBnGrnds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the final policy (could be slow because it renders images. Be patient)\n",
        "print (f\"Final Eval: {ac.eval(10, visualize=True):.2f}\")\n",
        "ac.visualize()"
      ],
      "metadata": {
        "id": "rNz6Vohar8g3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}