{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dane-meister/Machine-Learning-Algos/blob/main/dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_v04AsuPAA"
      },
      "source": [
        "# DQN\n",
        "\n",
        "The goal is to implement DQN algorithm.\n",
        "\n",
        "- We will be using 4 Gymnasium environments, 2 state-based (Cartpole-v1 and MountainCar-v0) and 2 image-based (ALE/Breakout-v5 and ALE/Boxing-v5).\n",
        "- We will use epsilon-greedy with epsilon-decay for the off-policy.\n",
        "- We will use an MLP to implement Q network for state-based environemnts, and CNN for image-based environments.\n",
        "- We will need the **GPU runtime** of Colab. Make sure you are on it. Please be patient when training on image-based environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAKjHf2vs1tP"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[classic-control,atari,accept-rom-license]\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRI7aIiMywhA"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from io import BytesIO\n",
        "\n",
        "# Code for visualizing the episode\n",
        "\n",
        "class GIFMaker:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.images = []\n",
        "        self.buffer = BytesIO()\n",
        "\n",
        "    def append(self, img):\n",
        "        self.images.append(img)\n",
        "\n",
        "    def display(self):\n",
        "        imageio.mimsave(self.buffer, self.images, format='gif')\n",
        "        gif = Image(data=self.buffer.getvalue())\n",
        "        display(gif)\n",
        "        return gif\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWjhkm1uV7N"
      },
      "source": [
        "## Env1 CartPole-v1\n",
        "Detailed information of this environment:\n",
        "https://gymnasium.farama.org/environments/classic_control/cart_pole/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L5xY5Rltb2o"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "g = GIFMaker() # visualization\n",
        "observation, info = env.reset(seed=42)\n",
        "for i in range(500):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # print(i, action, observation, reward)\n",
        "    g.append(env.render()) # save one frame\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "g.display() # show GIF animation\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zG0W9NjWGu"
      },
      "source": [
        "## Env2 MountainCar-v0\n",
        "Detailed information of this environment: https://gymnasium.farama.org/environments/classic_control/mountain_car/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLJpOasGjW2V"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "observation, info = env.reset(seed=42)\n",
        "g = GIFMaker()\n",
        "for i in range(200):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # print(i, action, observation, reward)\n",
        "    g.append(env.render())\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "g.display()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3AT79O_uxyT"
      },
      "source": [
        "## Env3 Breakout, an Atari game\n",
        "Detailed information of this environment: https://gymnasium.farama.org/environments/atari/breakout/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXsQceAQu-KA"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
        "\n",
        "observation, info = env.reset(seed=42)\n",
        "g = GIFMaker()\n",
        "for i in range(200):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # print(i, action, observation, reward)\n",
        "    g.append(env.render())\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "g.display()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nxHaf9wjmDj"
      },
      "source": [
        "## Env4 Boxing, an Atari game\n",
        "Detailed information of this environment: https://gymnasium.farama.org/environments/atari/breakout/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOF9pVAojjc-"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"ALE/Boxing-v5\", render_mode=\"rgb_array\")\n",
        "\n",
        "observation, info = env.reset(seed=42)\n",
        "g = GIFMaker()\n",
        "for i in range(200):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    # print(i, action, observation, reward)\n",
        "    g.append(env.render())\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "        break\n",
        "g.display()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfVZBRFwBjz8"
      },
      "source": [
        "##Import packages we need\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vasknJoRWy0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jnEgzV2uCD0"
      },
      "source": [
        "##Replay Buffer\n",
        "Implemented a replay buffer with size n. (Different task/environment may use buffers in different sizes.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCTx3tsMuBZc"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, n):\n",
        "        self.buffer = deque([], maxlen=n)\n",
        "\n",
        "\n",
        "    def add(self, x):\n",
        "        self.buffer.append(x) # add sample x to the buffer\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return []  # Return empty if not enough samples\n",
        "        return random.sample(self.buffer, batch_size) # return a list of \"packages\" before\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5fPlLjSW7y"
      },
      "source": [
        "##DQN for numerical states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egLnG4J4SWDd"
      },
      "outputs": [],
      "source": [
        "# MLP for Q function network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim\n",
        "    ):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Define the layers of the MLP\n",
        "        self.fc1 = nn.Linear(state_dim, 128)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(128, 64)         # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(64, action_dim)  # Output layer\n",
        "        # the output dimension should be action_dim\n",
        "        # which gives us Q values for all actions\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation function after first layer\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation function after second layer\n",
        "        x = self.fc3(x)          # Output layer (no activation function)\n",
        "\n",
        "        return x\n",
        "\n",
        "# DQN\n",
        "class DQN():\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        env_name,\n",
        "        eps=0.1,\n",
        "        eps_decay_steps=20000,\n",
        "        batch_size=64,\n",
        "        train_freq=4,\n",
        "        target_network_update_freq=200,\n",
        "        train_start=2000,\n",
        "        gamma=0.95,\n",
        "    ):\n",
        "        # initlize env and env name\n",
        "        self.env = env\n",
        "        self.env_name = env_name\n",
        "\n",
        "        # set state dimension and number of actions\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.n\n",
        "\n",
        "        # create two networks: 1) Policy (online) network, 2) Target network\n",
        "        self.policy_network = QNetwork(self.state_dim, self.action_dim)\n",
        "        self.target_network = QNetwork(self.state_dim, self.action_dim)\n",
        "\n",
        "\n",
        "        # make target network paramters same as policy network\n",
        "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "\n",
        "        # assign gpu if avliable\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # move the networks to device/gpu\n",
        "        self.policy_network = self.policy_network.to(self.device)\n",
        "        self.target_network = self.target_network.to(self.device)\n",
        "\n",
        "        # Create Replay Buffer with size 50000\n",
        "        self.rbuff = ReplayBuffer(50000)\n",
        "\n",
        "        # (end) epsilon value for epsilon off policy (self.eps)\n",
        "        self.eps = eps\n",
        "        # how many steps epsilon decays from 1 to eps\n",
        "        self.eps_decay_steps = eps_decay_steps\n",
        "\n",
        "        # set optimizer and loss function\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters())\n",
        "        self.loss_function = nn.MSELoss()\n",
        "\n",
        "        # batch size for sampling data from replay buffer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # mode is used to switch between optimization of DQN and Evaluation\n",
        "        self.mode = \"eval\"\n",
        "        self.total_transitions = 0\n",
        "        self.train_freq = train_freq\n",
        "        self.target_network_update_freq = target_network_update_freq\n",
        "        self.train_start = train_start\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.g = None # for visualization\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            if self.mode == \"eval\":\n",
        "                # state_tensor = torch.tensor([state], device=self.device, dtype=torch.float32)\n",
        "                q_values = self.policy_network(state)\n",
        "                action = torch.argmax(q_values, dim=1).unsqueeze(0)\n",
        "                return action\n",
        "            elif self.mode == \"train\":\n",
        "                if self.total_transitions < self.train_start:\n",
        "                    # before actually learning, do random actions\n",
        "                    action = torch.randint(0, self.action_dim, (state.size(0),), device=self.device, dtype=torch.long).unsqueeze(0)\n",
        "                    return action\n",
        "\n",
        "                # actual eps value for epsilon greedy\n",
        "                # here we consider a linear epsilon decay\n",
        "                # the actual eps value is 1 when total_transition is 0\n",
        "                # the actual eps value is self.eps if it is greater than self.eps_decay_steps (i.e. the end of decaying)\n",
        "                # in between them, just linearly decay the eps value according to how many steps (self.total_transitions) this DQN runs.\n",
        "                if self.total_transitions > self.eps_decay_steps:\n",
        "                    eps = self.eps\n",
        "                else:\n",
        "                    eps = 1 - (self.total_transitions / self.eps_decay_steps) * (1 - self.eps)\n",
        "\n",
        "                # epsilon-greedy policy\n",
        "                if np.random.rand() < eps:\n",
        "                    action = torch.randint(0, self.action_dim, (state.size(0),), device=self.device, dtype=torch.long).unsqueeze(0)\n",
        "                else:\n",
        "                    q_values = self.policy_network(state)\n",
        "                    action = torch.argmax(q_values, dim=1).unsqueeze(0)\n",
        "\n",
        "                return action\n",
        "\n",
        "\n",
        "    # optimizae the DQN\n",
        "    def optimization(self):\n",
        "        # make sure the policy network is train mode\n",
        "        self.policy_network.train()\n",
        "\n",
        "        # get a batch of state, action, next state, and reward and append them to convert to batch of tensors\n",
        "        state, action, next_state, reward, not_done = list(zip(*self.rbuff.sample(self.batch_size)))\n",
        "        state = torch.cat(state, dim=0).to(self.device)\n",
        "        action = torch.cat(action, dim=0).to(self.device)\n",
        "        next_state = torch.cat(next_state, dim=0).to(self.device)\n",
        "        reward = torch.cat(reward, dim=0).to(self.device)\n",
        "        not_done = torch.cat(not_done, dim=0).to(self.device)\n",
        "\n",
        "        # Q value calculation, loss compute, and backpropagation\n",
        "        q_values = self.policy_network(state)\n",
        "        q_values = q_values.gather(1, action)\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_network(next_state)\n",
        "            max_next_q_values = torch.max(next_q_values, dim=1).values\n",
        "            target_q_values = reward + self.gamma * not_done * max_next_q_values\n",
        "        loss = self.loss_function(q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.total_transitions % self.target_network_update_freq == 0:\n",
        "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "\n",
        "\n",
        "    # run for one episode\n",
        "    def run_one_episode(self):\n",
        "        self.g = GIFMaker()\n",
        "\n",
        "        # batch_size for training network\n",
        "        batch_size = 64\n",
        "\n",
        "        # initialize some values to track the end of current episode and cummulative reward\n",
        "        terminated = False\n",
        "        r = 0\n",
        "\n",
        "        state, info = self.env.reset()\n",
        "        # reset the environment\n",
        "        self.g.append(env.render())\n",
        "\n",
        "        while not terminated:\n",
        "            # create state_tensor that converts from state, and make a batch dimension, move it to gpu\n",
        "            state_tensor = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "            # create action_tensor which we follow the epsilon policy, use act()\n",
        "            action_tensor = self.act(state_tensor)\n",
        "            action = action_tensor.item()\n",
        "            # take that action in the environment and observe the next state\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "            # we need to use .item() on action_tensor to make it a number for the environment\n",
        "\n",
        "            r += reward\n",
        "            self.g.append(env.render())\n",
        "\n",
        "            # create next_state_tensor as a tensor\n",
        "            next_state_tensor = torch.tensor(next_state, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "            # create reward_tensor as a tensor\n",
        "            reward_tensor = torch.tensor(reward, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "            # create a binary (0, 1) variable for not_done: 1 for not done and 0 for done\n",
        "            if not terminated:\n",
        "                not_done = torch.tensor([1.0], device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "            else:\n",
        "                not_done = torch.tensor([0.0], device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            if self.mode == \"train\":\n",
        "                self.total_transitions += 1\n",
        "\n",
        "\n",
        "                # add state, action, next state and reward tensors to buffer, using the add() method of the buffer class\n",
        "                transition_package = [state_tensor.cpu(), action_tensor.cpu(), next_state_tensor.cpu(), reward_tensor.cpu(), not_done.cpu()]\n",
        "                self.rbuff.add(transition_package)\n",
        "\n",
        "                if len(self.rbuff) < self.train_start:\n",
        "                    # don't train before train_start, just gathering more samples\n",
        "                    continue\n",
        "                else:\n",
        "                    # we optimize the network for every other self.train_freq steps\n",
        "                    if self.total_transitions % self.train_freq == 0:\n",
        "                        self.optimization()\n",
        "\n",
        "            # Terminated then return\n",
        "            if terminated or truncated:\n",
        "                return r\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        return r\n",
        "\n",
        "    def train(self):\n",
        "        self.mode = \"train\"\n",
        "        r = self.run_one_episode()\n",
        "        # print(f\"Training Reward: {r:.2f}\") # optionally print out reward of this episode\n",
        "\n",
        "    # Evaluation of policy\n",
        "    def eval(self, n):\n",
        "        self.mode = \"eval\" # put in eval mode\n",
        "        returns = []\n",
        "        for i in range(n):                          # run evaluation for n episode\n",
        "            returns.append(self.run_one_episode())\n",
        "        return np.mean(returns)                    # return average returns over niter episodes\n",
        "\n",
        "\n",
        "    # the function called to perform optimization and evaluation\n",
        "    def execute(self, total_ep=5000, eval_freq=100, eval_ep=100):\n",
        "        rewards = []   # used to track polciy evaluation across runs\n",
        "        episodes = []  # number of episodes used to update policy\n",
        "\n",
        "        prog_bar = tqdm(range(0, total_ep))\n",
        "        for i in prog_bar:\n",
        "            self.train()                        # train\n",
        "            if (i+1) % eval_freq == 0:          # evaluate using eval_ep episodes every eval_freq policy updates\n",
        "                reward = self.eval(eval_ep)\n",
        "                print (f\"Eval Reward: {reward:.2f}\")\n",
        "                rewards.append(reward)\n",
        "                episodes.append(i)\n",
        "\n",
        "        plt.plot(episodes, rewards)   # plot evaluation reward vs episodes\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('DQN on '+self.env_name)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize(self):\n",
        "        self.g.display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nTPuXmz8nYal"
      },
      "outputs": [],
      "source": [
        "# Cart Pole DQN\n",
        "gc.collect() # free some unused RAM, don't worry about this\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "dqn_cartpole = DQN(env, \"cartpole\", eps=0.01, batch_size=64)\n",
        "dqn_cartpole.execute(total_ep=500, eval_freq=100, eval_ep=100) # total_ep=5000 will take ~25 mins on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VcNniVZQ8pNC"
      },
      "outputs": [],
      "source": [
        "cartpole_eval = dqn_cartpole.eval(100)\n",
        "print(f\"CartPole Final Evaluation: {cartpole_eval}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SHvZ7aFE5yhI"
      },
      "outputs": [],
      "source": [
        "# Mountain Car DQN\n",
        "gc.collect() # free some unused RAM, don't worry about this\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "dqn_mountaincar = DQN(env, \"mountaincar\", eps=0.01, batch_size=64)\n",
        "dqn_mountaincar.execute(total_ep=200, eval_freq=100, eval_ep=20) # 2000 episodes take ~34 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nx3leWic8yLe",
        "outputId": "972154ac-e235-473e-bc98-cf6fcd6bd110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MountainCar Final Evaluation: -200.0\n"
          ]
        }
      ],
      "source": [
        "mountaincar_eval = dqn_mountaincar.eval(20)\n",
        "print(f\"MountainCar Final Evaluation: {mountaincar_eval}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1oT97j8jKCp"
      },
      "source": [
        "#DQN for image states (observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeUzjPXJjJAq"
      },
      "outputs": [],
      "source": [
        "class QNetwork_Image(nn.Module):\n",
        "    def __init__(self, num_actions, num_channels):\n",
        "        super(QNetwork_Image, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1,6,5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        self.fc1 = nn.Linear(5184, 128)\n",
        "        self.fc2 = nn.Linear(128, 84)\n",
        "        self.fc3 = nn.Linear(84, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.reshape(x, (x.size(0),-1) )\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def resize_and_normalize(state, image_size=(84, 84)):\n",
        "    resized_img = cv2.resize(state, image_size)\n",
        "    normalized_img = resized_img.astype(np.float32) / 255.0\n",
        "    if len(normalized_img.shape) == 2:  # Grayscale\n",
        "        normalized_img = normalized_img.reshape((1, image_size[0], image_size[1]))\n",
        "    else:\n",
        "        normalized_img = np.transpose(normalized_img, (2, 0, 1))\n",
        "    return normalized_img\n",
        "\n",
        "class DQN_Image():\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        env_name,\n",
        "        eps=0.1,\n",
        "        eps_decay_steps=20000,\n",
        "        batch_size=64,\n",
        "        train_freq=4,\n",
        "        target_network_update_freq=200,\n",
        "        train_start=2000,\n",
        "        gamma=0.95,\n",
        "    ):\n",
        "        # initlize env and env name\n",
        "        self.env = env\n",
        "        self.env_name = env_name\n",
        "         # set state dimension and number of actions\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.n\n",
        "\n",
        "        # create two networks: 1) Policy (online) network, 2) Target network\n",
        "        self.policy_network = QNetwork_Image(self.action_dim, 1)\n",
        "        self.target_network = QNetwork_Image(self.action_dim, 1)\n",
        "        # make target network paramters same as policy network\n",
        "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        # assign gpu if avliable\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # move the networks to device/gpu\n",
        "        self.policy_network.to(self.device)\n",
        "        self.target_network.to(self.device)\n",
        "        # Create Replay Buffer with size 50000\n",
        "        self.rbuff = ReplayBuffer(50000)\n",
        "\n",
        "        # (end) epsilon value for epsilon off policy (self.eps)\n",
        "        self.eps = eps\n",
        "        # how many steps epsilon decays from 1 to eps\n",
        "        self.eps_decay_steps = eps_decay_steps\n",
        "\n",
        "        # set optimizer and loss function\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.001)\n",
        "        self.loss_func = nn.MSELoss()\n",
        "\n",
        "        # batch size for sampling data from replay buffer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # mode is used to switch between optimization of DQN and Evaluation\n",
        "        self.mode = \"eval\"\n",
        "        self.total_transitions = 0\n",
        "        self.train_freq = train_freq\n",
        "        self.target_network_update_freq = target_network_update_freq\n",
        "        self.train_start = train_start\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.g = None # for visualization\n",
        "          # function to choose action given a state\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            if self.mode == \"eval\":\n",
        "                q_values = self.policy_network(state)\n",
        "                action = torch.argmax(q_values, dim=1).unsqueeze(0)\n",
        "                return action\n",
        "            elif self.mode == \"train\":\n",
        "                if self.total_transitions < self.train_start:\n",
        "                    # before actually learning, do random actions\n",
        "                    action = torch.randint(0, self.action_dim, (state.size(0),), device=self.device, dtype=torch.long).unsqueeze(0)\n",
        "                    return action\n",
        "\n",
        "                # actual eps value for epsilon greedy\n",
        "                # here we consider a linear epsilon decay\n",
        "                # the actual eps value is 1 when total_transition is 0\n",
        "                # the actual eps value is self.eps if it is greater than self.eps_decay_steps (i.e. the end of decaying)\n",
        "                # in between them, just linearly decay the eps value according to how many steps (self.total_transitions) this DQN runs.\n",
        "                if self.total_transitions <= self.eps_decay_steps:\n",
        "                    eps = 1 + ((self.eps - 1)/(self.eps_decay_steps))*self.total_transitions\n",
        "                else:\n",
        "                    eps = self.eps\n",
        "\n",
        "\n",
        "                # epsilon-greedy policy\n",
        "                # return an action according to eps\n",
        "                if np.random.rand() < eps:\n",
        "                    action = torch.randint(0, self.action_dim, (state.size(0),), device=self.device, dtype=torch.long).unsqueeze(0)\n",
        "                else:\n",
        "                    q_values = self.policy_network(state)\n",
        "                    action = torch.argmax(q_values, dim=1).unsqueeze(0)\n",
        "\n",
        "                return action\n",
        "\n",
        "    # optimizae the DQN\n",
        "    def optimization(self):\n",
        "        # make sure the policy network is train mode\n",
        "        self.policy_network.train()\n",
        "\n",
        "        # get a batch of state, action, next state, and reward and append them to convert to batch of tensors\n",
        "        state, action, next_state, reward, not_done = list(zip(*self.rbuff.sample(self.batch_size)))\n",
        "\n",
        "        state = torch.cat(state, dim=0).to(self.device)\n",
        "        # given the example of state, please complete what should we do for action, next_state, reward, and not_done\n",
        "        action = torch.cat(action, dim=0).to(self.device)\n",
        "        next_state = torch.cat(next_state, dim=0).to(self.device)\n",
        "        reward = torch.cat(reward, dim=0).to(self.device)\n",
        "        not_done = torch.cat(not_done, dim=0).to(self.device)\n",
        "\n",
        "        q_values = self.policy_network(state)\n",
        "        q_values = q_values.gather(1, action)\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_network(next_state)\n",
        "            max_next_q_values = torch.max(next_q_values, dim=1).values\n",
        "            target_q_values = reward + self.gamma * not_done * max_next_q_values\n",
        "        loss = self.loss_func(q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update the target network based on new policy network paramters\n",
        "        if self.total_transitions % self.target_network_update_freq == 0:\n",
        "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    # run for one episode\n",
        "    def run_one_episode(self):\n",
        "        self.g = GIFMaker()\n",
        "\n",
        "        # batch_size for training network\n",
        "        batch_size = 64\n",
        "\n",
        "        # initialize some values to track the end of current episode and cummulative reward\n",
        "        terminated = False\n",
        "        r = 0\n",
        "\n",
        "\n",
        "        # reset the environment\n",
        "        state,info = self.env.reset()\n",
        "        state = resize_and_normalize(state)\n",
        "        self.g.append(env.render())\n",
        "\n",
        "        while not terminated:\n",
        "            # create state_tensor that converts from state, and make a batch dimension, move it to gpu\n",
        "            # create action_tensor which we follow the epsilon policy, use act()\n",
        "            # take that action in the environment and observe the next state\n",
        "\n",
        "            state_tensor = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "            action_tensor = self.act(state_tensor)\n",
        "            action = action_tensor.item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "            next_state = resize_and_normalize(next_state)\n",
        "\n",
        "            r += reward\n",
        "            self.g.append(env.render())\n",
        "\n",
        "\n",
        "            # create next_state_tensor as a tensor\n",
        "            # create reward_tensor as a tensor\n",
        "            # create a binary (0, 1) variable for not_done: 1 for not done and 0 for done\n",
        "            next_state_tensor = torch.tensor(next_state, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "            reward_tensor = torch.tensor(reward, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            if not terminated:\n",
        "                not_done = torch.tensor([1.0], device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "            else:\n",
        "                not_done = torch.tensor([0.0], device=self.device, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "\n",
        "            if self.mode == \"train\":\n",
        "                self.total_transitions += 1\n",
        "                # add state, action, next state and reward tensors to buffer, using the add() method of the buffer class\n",
        "                transition_package = [state_tensor.cpu(), action_tensor.cpu(), next_state_tensor.cpu(), reward_tensor.cpu(), not_done.cpu()]\n",
        "                self.rbuff.add(transition_package)\n",
        "\n",
        "                if len(self.rbuff) < self.train_start:\n",
        "                    # don't train before train_start, just gathering more samples\n",
        "                    continue\n",
        "                else:\n",
        "                    # we optimize the network for every other self.train_freq steps\n",
        "                    if self.total_transitions % self.train_freq == 0:\n",
        "                        loss = self.optimization()\n",
        "\n",
        "            # Terminated then return\n",
        "            if terminated or truncated:\n",
        "                return r\n",
        "\n",
        "            # assign next_state to state for the next loop\n",
        "            state = next_state\n",
        "\n",
        "    def train(self):\n",
        "        self.mode = \"train\"\n",
        "        r = self.run_one_episode()\n",
        "        # print(f\"Training Reward: {r:.2f}\") # optionally print out reward of this episode in case you are boring\n",
        "\n",
        "    # Evaluation of policy\n",
        "    def eval(self, n):\n",
        "        self.mode = \"eval\" # put in eval mode\n",
        "        returns = []\n",
        "        for i in range(n):                          # run evaluation for n episode\n",
        "            returns.append(self.run_one_episode())\n",
        "        return np.mean(returns)                    # return average returns over niter episodes\n",
        "\n",
        "\n",
        "    # the function called to perform optimization and evaluation\n",
        "    def execute(self, total_ep=5000, eval_freq=100, eval_ep=100):\n",
        "        rewards = []   # used to track polciy evaluation across runs\n",
        "        episodes = []  # number of episodes used to update policy\n",
        "\n",
        "        prog_bar = tqdm(range(0, total_ep))\n",
        "        for i in prog_bar:\n",
        "            self.train()                        # train\n",
        "            print(i)\n",
        "            if (i+1) % eval_freq == 0:          # evaluate using eval_ep episodes every eval_freq policy updates\n",
        "                reward = self.eval(eval_ep)\n",
        "                print (f\"Eval Reward: {reward:.2f}\")\n",
        "                rewards.append(reward)\n",
        "                episodes.append(i)\n",
        "\n",
        "        plt.plot(episodes, rewards)   # plot evaluation reward vs episodes\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('DQN on '+self.env_name)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize(self):\n",
        "        self.g.display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll403f7KruOV"
      },
      "outputs": [],
      "source": [
        "# Breakout DQN\n",
        "# breakout could be a bit hard to learn\n",
        "gc.collect()\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\", obs_type=\"grayscale\") # optionally you can try obs_type=rgb, which gives you rgb images. if using rgb images, you may need to adjust the size of replay buffer to avoid out-of-memory (RAM)\n",
        "dqn_image = DQN_Image(env, \"breakout\", eps=0.01, batch_size=64)\n",
        "dqn_image.execute(total_ep=850, eval_freq=100, eval_ep=10) # 1000 episodes takes ~60mins on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0mPULXs8zB0"
      },
      "outputs": [],
      "source": [
        "breakout_eval = dqn_image.eval(20)\n",
        "print(f\"Breakout Final Evaluation: {breakout_eval}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OztKVJ54mFme"
      },
      "outputs": [],
      "source": [
        "# Boxing DQN\n",
        "gc.collect()\n",
        "env = gym.make(\"ALE/Boxing-v5\", render_mode=\"rgb_array\", obs_type=\"grayscale\") # optionally you can try obs_type=rgb, which gives you rgb images. if using rgb images, you may need to adjust the size of replay buffer to avoid out-of-memory (RAM)\n",
        "dqn_image = DQN_Image(env, \"boxing\", eps=0.01, batch_size=64)\n",
        "dqn_image.execute(total_ep=500, eval_freq=100, eval_ep=10) # 500 episodes takes ~53mins on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3aftD7fU8zZv"
      },
      "outputs": [],
      "source": [
        "boxing_eval = dqn_image.eval(20)\n",
        "print(f\"Boxing Final Evaluation: {boxing_eval}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}